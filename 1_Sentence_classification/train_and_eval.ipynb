{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH32rzgprvgM"
      },
      "source": [
        "# Import requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9EvC1HBuf41",
        "outputId": "e8ddefc1-a6d2-4b13-f31f-8ec340bf05e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 15.5 MB/s \n",
            "\u001b[?25hCollecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 86.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 83.1 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 62.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 88.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown>=4.0.0->nlpaug) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2022.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.8.0-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 82.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.7.1)\n",
            "Building wheels for collected packages: sacremoses, pathtools\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fcb8cc2ac6bccd96279d69e519b59aafddff7f8c8246ee8f8242ee4a3be74aa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=9b68e9888dc43835d8ee5c847cb54eb34e34bc529dfa461cf94d5d8853cb2216\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built sacremoses pathtools\n",
            "Installing collected packages: smmap, pyyaml, gitdb, tokenizers, shortuuid, setproctitle, sentry-sdk, pathtools, huggingface-hub, GitPython, docker-pycreds, wandb, transformers, sentencepiece, sacremoses, nlpaug\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 huggingface-hub-0.8.1 nlpaug-1.1.11 pathtools-0.1.2 pyyaml-6.0 sacremoses-0.0.53 sentencepiece-0.1.96 sentry-sdk-1.8.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 tokenizers-0.12.1 transformers-4.20.1 wandb-0.12.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        }
      ],
      "source": [
        "! pip install transformers nlpaug sacremoses wandb sentencepiece\n",
        "! wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUqDXSLg9P8y",
        "outputId": "c60ada7c-d4bf-47cc-d8d3-16cc71757321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(os.path.join('/', 'content', 'drive', 'My Drive', 'goorm K-Digital', '자연어처리', 'Project1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAdLxrUZrvgP",
        "outputId": "09d7e28c-5a55-4579-953a-7aa5284c1a69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5323fc37b0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pdb\n",
        "import argparse\n",
        "import random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    DebertaV2ForSequenceClassification,\n",
        "    DebertaV2TokenizerFast,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "import wandb\n",
        "\n",
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# random seed\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCOnw_3_wolb"
      },
      "outputs": [],
      "source": [
        "class ProjectDataLoader(object):\n",
        "    def __init__(self, file_list, aug=None):\n",
        "        self.file_list = file_list\n",
        "        self.dataset = {}\n",
        "        self.load()\n",
        "        if aug:\n",
        "            print(f'Wait for loading augmenter...')\n",
        "            self.augmenter = self.load_augmenter()\n",
        "        else:\n",
        "            self.augmenter = None\n",
        "\n",
        "    def load(self):\n",
        "        print(f'Load datasets from {self.file_list}')\n",
        "        for file_path in self.file_list:\n",
        "            data_type, label = self.get_info_from_name(file_path)\n",
        "            label = int(label)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                self.dataset[file_path] = list(map(lambda x: x.strip(), lines))\n",
        "\n",
        "    def load_augmenter(self):\n",
        "        augmenter = [\n",
        "            naw.BackTranslationAug(\n",
        "                from_model_name='facebook/wmt19-en-de',\n",
        "                to_model_name='facebook/wmt19-de-en',\n",
        "                device='cuda'\n",
        "            ),\n",
        "            naw.ContextualWordEmbsAug(\n",
        "                model_path='bert-base-uncased', # 'distilbert-base-uncased' or 'roberta-base available'\n",
        "                action='insert',\n",
        "                device='cuda'\n",
        "            ),\n",
        "            naw.ContextualWordEmbsAug(\n",
        "                model_path='bert-base-uncased',\n",
        "                action='substitute',\n",
        "                device='cuda'\n",
        "            ),\n",
        "            naw.SynonymAug(aug_src='wordnet'),\n",
        "            naw.RandomWordAug(action='swap')\n",
        "        ]\n",
        "        return augmenter\n",
        "\n",
        "    def get_info_from_name(self, name):\n",
        "        data_type, label = name.split('.')[-2:]\n",
        "        return data_type, label\n",
        "\n",
        "    # Drop duplicated data.\n",
        "    def drop_duplicated(self, verbose=False):\n",
        "        print('-'*30)\n",
        "        print('> Drop duplicated data')\n",
        "        for name, dataset in self.dataset.items():\n",
        "            print('-'*30)\n",
        "            print(f'{name}')\n",
        "            if 'dev' in name:\n",
        "                train_set = set(self.dataset[name.replace('dev', 'train')])\n",
        "                orig_len = len(dataset)\n",
        "                val_set = set(dataset)\n",
        "                val_set = val_set - train_set\n",
        "                dataset = list(val_set)\n",
        "                drop_len = len(dataset)\n",
        "                self.dataset[name] = dataset\n",
        "                print(f'drop duplicated with train: {orig_len:,} -> {drop_len:,}')\n",
        "\n",
        "            orig_len = len(dataset)\n",
        "            set_dataset = set(dataset)\n",
        "            drop_len = len(set_dataset)\n",
        "            num_duplicated = orig_len - drop_len\n",
        "            \n",
        "            print(f'duplicated : total / {num_duplicated:,} : {orig_len:,}')\n",
        "            self.dataset[name] = list(set_dataset)\n",
        "            print(f'{len(self.dataset[name]):,} sentences exist in {name}.')\n",
        "\n",
        "    # Data augmentation\n",
        "    def augment(self, ratio, test=False):\n",
        "        if not self.augmenter:\n",
        "            print(f'Augmenter is not exist.')\n",
        "            return \n",
        "        \n",
        "        result = []\n",
        "        for name, dataset in self.dataset.items():\n",
        "            orig_len = len(dataset)\n",
        "            data_type, label = self.get_info_from_name(name)\n",
        "            if data_type != 'train':\n",
        "                continue\n",
        "            print('-'*30)\n",
        "            print(f'{name}')\n",
        "            \n",
        "            num_sentences = len(dataset)\n",
        "            num_aug_per = int(num_sentences * ratio / len(self.augmenter))\n",
        "            print(f'{num_aug_per * len(self.augmenter):,} augmentated data will be added.')\n",
        "            \n",
        "            sampled_sentences = random.sample(dataset, num_aug_per * len(self.augmenter))\n",
        "            sampled_sentences = self.list_chunk(sampled_sentences, num_aug_per)\n",
        "            aug_sentences = []\n",
        "            for idx, sentences in tqdm(zip(list(range(len(self.augmenter))), sampled_sentences), total=len(sampled_sentences), desc='augmentation'):\n",
        "                aug_sentence = self.augmenter[idx].augment(sentences)\n",
        "                aug_sentences += aug_sentence\n",
        "                if test:              \n",
        "                    result.append((idx, sentences, aug_sentence))\n",
        "\n",
        "            self.dataset[name] += aug_sentences\n",
        "            print(f'total: {orig_len:,} -> {len(self.dataset[name]):,}')\n",
        "        if test:\n",
        "            return result\n",
        "        \n",
        "    # Important part... We used this code before. But this method require dataset's name.\n",
        "    def make_id_file(self, name, tokenizer):\n",
        "        print(f'tokenizing {name}')\n",
        "        data_strings = []\n",
        "        id_file_data = [tokenizer.encode(line.lower()) for line in self.dataset[name]]\n",
        "        for item in id_file_data:\n",
        "            data_strings.append(' '.join([str(k) for k in item]))\n",
        "        return data_strings\n",
        "\n",
        "    def list_chunk(self, arr, n):\n",
        "        return [arr[i: i + n] for i in range(0, len(arr), n)]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx]\n",
        "\n",
        "    # You don't need to look this method. This method shows information about our datasets.\n",
        "    def summary(self):\n",
        "        print('-'*30)\n",
        "        print('> Smmary')\n",
        "        for name, dataset in self.dataset.items():\n",
        "            data_type, label = self.get_info_from_name(name)\n",
        "            num_of_sentences = len(dataset)\n",
        "            print('-'*30)\n",
        "            print(f'[{name}]')\n",
        "            print(f'number of sentences: {num_of_sentences:,}')\n",
        "            print(f'dataset type: {data_type}')\n",
        "            print(f'label: {label}')\n",
        "\n",
        "class SentimentDataset(object):\n",
        "    def __init__(self, tokenizer, pos, neg):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for pos_sent in pos:\n",
        "            self.data += [self._cast_to_int(pos_sent.strip().split())]\n",
        "            self.label += [[1]]\n",
        "        for neg_sent in neg:\n",
        "            self.data += [self._cast_to_int(neg_sent.strip().split())]\n",
        "            self.label += [[0]]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample), np.array(self.label[index])\n",
        "\n",
        "def compute_acc(predictions, target_labels):\n",
        "    return (np.array(predictions) == np.array(target_labels)).mean()\n",
        "\n",
        "class WeightedFocalLoss(torch.nn.Module):\n",
        "    def __init__(self, alpha=.25, gamma=2):\n",
        "        super(WeightedFocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        targets = targets.type(torch.long)\n",
        "        at = self.alpha.gather(0, targets.data.view(-1))\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u-95KigwqOn",
        "outputId": "f24e61eb-c91b-4d0a-b88b-87e5826d164c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load datasets from ['sentiment.train.0', 'sentiment.train.1', 'sentiment.dev.0', 'sentiment.dev.1']\n",
            "------------------------------\n",
            "> Smmary\n",
            "------------------------------\n",
            "[sentiment.train.0]\n",
            "number of sentences: 177,218\n",
            "dataset type: train\n",
            "label: 0\n",
            "------------------------------\n",
            "[sentiment.train.1]\n",
            "number of sentences: 266,041\n",
            "dataset type: train\n",
            "label: 1\n",
            "------------------------------\n",
            "[sentiment.dev.0]\n",
            "number of sentences: 2,000\n",
            "dataset type: dev\n",
            "label: 0\n",
            "------------------------------\n",
            "[sentiment.dev.1]\n",
            "number of sentences: 2,000\n",
            "dataset type: dev\n",
            "label: 1\n",
            "------------------------------\n",
            "> Drop duplicated data\n",
            "------------------------------\n",
            "sentiment.train.0\n",
            "duplicated : total / 19,437 : 177,218\n",
            "157,781 sentences exist in sentiment.train.0.\n",
            "------------------------------\n",
            "sentiment.train.1\n",
            "duplicated : total / 43,167 : 266,041\n",
            "222,874 sentences exist in sentiment.train.1.\n",
            "------------------------------\n",
            "sentiment.dev.0\n",
            "drop duplicated with train: 2,000 -> 1,726\n",
            "duplicated : total / 0 : 1,726\n",
            "1,726 sentences exist in sentiment.dev.0.\n",
            "------------------------------\n",
            "sentiment.dev.1\n",
            "drop duplicated with train: 2,000 -> 1,577\n",
            "duplicated : total / 0 : 1,577\n",
            "1,577 sentences exist in sentiment.dev.1.\n"
          ]
        }
      ],
      "source": [
        "file_list = ['sentiment.train.0', \n",
        "             'sentiment.train.1', \n",
        "             'sentiment.dev.0', \n",
        "             'sentiment.dev.1']\n",
        "\n",
        "duplication_check = True\n",
        "data_augmentation = True\n",
        "aug_ratio = 0.4\n",
        "\n",
        "datasets = ProjectDataLoader(file_list, aug=False)\n",
        "datasets.summary()\n",
        "if duplication_check:\n",
        "    datasets.drop_duplicated()\n",
        "# if data_augmentation:\n",
        "#     aug_sentences = datasets.augment(aug_ratio, True)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMpkgfnd94jD",
        "outputId": "03c8a7e4-4be7-4802-c7c7-e81a9600ec45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model = DebertaV2ForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=1)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-base', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "UWgD_JFZu_BE",
        "outputId": "eb53b162-c9b4-40a9-f8b4-a1a7f8999da8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoowater\u001b[0m (\u001b[33mteam_koowater\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/goorm K-Digital/자연어처리/Project1/wandb/run-20220718_133403-19ws4bum</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/team_koowater/DeBERTaV3/runs/19ws4bum\" target=\"_blank\">dc_True-lr5e-05-scTrue-wd0.01-bs32-da0.4</a></strong> to <a href=\"https://wandb.ai/team_koowater/DeBERTaV3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/team_koowater/DeBERTaV3/runs/19ws4bum?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f527782ee50>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 5e-5\n",
        "betas = [0.9, 0.99]\n",
        "eps = 1e-6\n",
        "apply_scheduler = True\n",
        "weight_decay = 0.01 # Order to paper, proper weight_decay is 0.01\n",
        "optimizer = AdamW(model.parameters(), betas=betas, eps=eps, weight_decay=weight_decay, lr=learning_rate) \n",
        "train_epoch = 3\n",
        "train_batch_size = 32\n",
        "eval_batch_size = 32\n",
        "total_train_step = np.ceil((len(datasets['sentiment.train.0']) + len(datasets['sentiment.train.1'])) / train_batch_size) * train_epoch\n",
        "if apply_scheduler:\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, 1000, total_train_step)\n",
        "\n",
        "training_name = f'dc_{duplication_check}-lr{learning_rate}-sc{apply_scheduler}-wd{weight_decay}-bs{train_batch_size}-da{aug_ratio}'\n",
        "wandb.init(\n",
        "    entity='team_koowater',\n",
        "    project='DeBERTaV3',\n",
        "    name=training_name,\n",
        "    config={\n",
        "        'model': model.__class__.__name__,\n",
        "        'learning_rate': learning_rate,\n",
        "        'optimizer': optimizer.__class__.__name__,\n",
        "        'betas': betas,\n",
        "        'eps': eps,\n",
        "        'weight_decay': weight_decay,\n",
        "        'scheduler': apply_scheduler,\n",
        "        'duplicated_check': duplication_check,\n",
        "        'data_augmentation': data_augmentation,\n",
        "        'aug_ratio': aug_ratio,\n",
        "        'train_epoch': train_epoch,\n",
        "        'train_batch_size': train_batch_size,\n",
        "        'val_batch_size': eval_batch_size\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWOOmXqrvgQ"
      },
      "source": [
        "# 1. Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSIt5GHeDlrv"
      },
      "source": [
        "## 고려하지 않는 augmentation\n",
        "\n",
        "- character-level \n",
        "\n",
        "## 적용 예정 augmentation\n",
        "\n",
        "- BackTranslationAug\n",
        "- `WordEmbsAug`: word2vec에 따라, 문장 내 선택된 특정 단어와 유사한 단어를 선택해 교체한다.\n",
        "- `TfIdfAug`: TF-IDF 유사도에 따라 단어를 삽입한다.\n",
        "- `ContextualWordEmbsAug`: (BERT, DistillBERT, RoBERTa, XLNet) 등 LM의 contextual word embeddings에 따라 단어를 삽입 또는 교체한다. \n",
        "- `SynonymAug`: (WordNet, PPDB)의 동의어에 따라 단어를 교체한다.\n",
        "\n",
        "## 고려 해야 할 augmentation\n",
        "\n",
        "- `RandomWordAug(swap)`: 문장 내 두 단어의 순서를 교체한다.\n",
        "- `RandomWordAug()`: 문장 내 단어들을 무작위로 삭제한다.\n",
        "- `RandomWordAug(crop)`: 문장 내 단어 덩어리를 무작위로 삭제한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39AEBbJl5Irk",
        "outputId": "7308fe7f-8957-43ad-ecb7-44420ebbe658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizing sentiment.train.1\n",
            "tokenizing sentiment.train.0\n",
            "tokenizing sentiment.dev.1\n",
            "tokenizing sentiment.dev.0\n"
          ]
        }
      ],
      "source": [
        "train_pos = datasets.make_id_file('sentiment.train.1', tokenizer)\n",
        "train_neg = datasets.make_id_file('sentiment.train.0', tokenizer)\n",
        "val_pos = datasets.make_id_file('sentiment.dev.1', tokenizer)\n",
        "val_neg = datasets.make_id_file('sentiment.dev.0', tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gS6c8xRUmY4"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFFFjaRAUosH"
      },
      "outputs": [],
      "source": [
        "# with open(\"sentiment_aug_4.train.1\", \"wb\") as fw:\n",
        "#     pickle.dump(train_pos, fw)\n",
        "# with open(\"sentiment_aug_4.train.0\", \"wb\") as fw:\n",
        "#     pickle.dump(train_neg, fw)\n",
        "# with open(\"sentiment_aug.dev.1\", \"wb\") as fw:\n",
        "#     pickle.dump(val_pos, fw)\n",
        "# with open(\"sentiment_aug.dev.0\", \"wb\") as fw:\n",
        "#     pickle.dump(val_neg, fw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgvTkh5EVagc"
      },
      "outputs": [],
      "source": [
        "with open(\"sentiment_aug_4.train.1\", \"rb\") as fr:\n",
        "    train_pos = pickle.load(fr)\n",
        "with open(\"sentiment_aug_4.train.0\", \"rb\") as fr:\n",
        "    train_neg = pickle.load(fr)\n",
        "# with open(\"sentiment.dev.1\", \"rb\") as fr:\n",
        "#     val_pos = pickle.load(fr)\n",
        "# with open(\"sentiment.dev.0\", \"rb\") as fr:\n",
        "#     val_neg = pickle.load(fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCz5ey8xrvgU"
      },
      "outputs": [],
      "source": [
        "train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\n",
        "val_dataset = SentimentDataset(tokenizer, val_pos, val_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0wRUBYSrvgU"
      },
      "outputs": [],
      "source": [
        "def collate_fn_style(samples):\n",
        "    input_ids, labels = zip(*samples)\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n",
        "\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tf9iWITsmPh"
      },
      "source": [
        "### Original paper hyperparameter\n",
        "\n",
        "- learning rate: 6e-4\n",
        "- optimizer: AdamW with weight decay, eps=1e-6, b1=0.9, b2=0.98\n",
        "- batch size: 8k\n",
        "- Weight decay: 0.01\n",
        "- Warmup step: 10k\n",
        "- Learning Rate Decay: Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5saagig0rvgV"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=train_batch_size,\n",
        "                                           shuffle=True, collate_fn=collate_fn_style,\n",
        "                                           pin_memory=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size,\n",
        "                                         shuffle=False, collate_fn=collate_fn_style,\n",
        "                                         num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuZfvzpGrvgW"
      },
      "outputs": [],
      "source": [
        "lowest_valid_loss = 9999.\n",
        "highest_acc = 0.\n",
        "\n",
        "for epoch in range(train_epoch):\n",
        "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "        losses = []\n",
        "        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            position_ids = position_ids.to(device)\n",
        "            labels = torch.squeeze(labels) # For DeBERTa\n",
        "            labels = labels.to(device, dtype=torch.float)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_ids=input_ids,\n",
        "                           attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids,\n",
        "                           position_ids=position_ids,\n",
        "                           labels=labels)\n",
        "            logits = output.logits\n",
        "            loss = output.loss\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if apply_scheduler:\n",
        "                scheduler.step()\n",
        "\n",
        "            # mini-batch의 loss만이 아니라 epoch의 total loss를 고려해서 출력해야한다.\n",
        "            tepoch.set_postfix(loss=np.mean(losses))\n",
        "            if iteration != 0 and iteration % int(len(train_loader) / 20) == 0:\n",
        "                # Evaluate the model five times per epoch\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    valid_losses = []\n",
        "                    predictions = []\n",
        "                    target_labels = []\n",
        "                    for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(val_loader,\n",
        "                                                                                                desc='Eval',\n",
        "                                                                                                position=1,\n",
        "                                                                                                leave=True):\n",
        "                        input_ids = input_ids.to(device)\n",
        "                        attention_mask = attention_mask.to(device)\n",
        "                        token_type_ids = token_type_ids.to(device)\n",
        "                        position_ids = position_ids.to(device)\n",
        "                        labels = torch.squeeze(labels) # For DeBERTa\n",
        "                        labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "                        output = model(input_ids=input_ids,\n",
        "                                       attention_mask=attention_mask,\n",
        "                                       token_type_ids=token_type_ids,\n",
        "                                       position_ids=position_ids,\n",
        "                                       labels=labels)\n",
        "\n",
        "                        logits = output.logits\n",
        "                        loss = output.loss\n",
        "                        valid_losses.append(loss.item())\n",
        "\n",
        "                        batch_predictions = [0 if logit < 0.5 else 1 for logit in logits]\n",
        "                        batch_labels = [int(example) for example in labels]\n",
        "\n",
        "                        predictions += batch_predictions\n",
        "                        target_labels += batch_labels\n",
        "                \n",
        "                acc = compute_acc(predictions, target_labels)\n",
        "                valid_loss = sum(valid_losses) / len(valid_losses)\n",
        "                wandb.log({\n",
        "                    'loss': np.mean(losses),\n",
        "                    'val_loss': np.mean(valid_losses),\n",
        "                    'acc': acc,\n",
        "                    'lr': optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "                })\n",
        "\n",
        "                losses = []\n",
        "                model.train()\n",
        "\n",
        "                if lowest_valid_loss > valid_loss or highest_acc < acc:\n",
        "                        \n",
        "                    print(f'Model saved - val_loss: {valid_loss}, acc: {acc}')\n",
        "                    if lowest_valid_loss > valid_loss:\n",
        "                        lowest_valid_loss = valid_loss\n",
        "                        torch.save(model.state_dict(), f\"./lowest_val_loss_{training_name}.bin\")\n",
        "                    if highest_acc < acc:\n",
        "                        highest_acc = acc\n",
        "                        torch.save(model.state_dict(), f\"./highest_acc_{training_name}.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnt693N0rvgX"
      },
      "outputs": [],
      "source": [
        "def make_id_file_test(tokenizer, test_dataset):\n",
        "    data_strings = []\n",
        "    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]\n",
        "    for item in id_file_data:\n",
        "        data_strings.append(' '.join([str(k) for k in item]))\n",
        "    return data_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3VBtly4XS2W"
      },
      "source": [
        "## Test and save csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P95gtlnurvgX"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(\n",
        "    torch.load('highest_acc_dc_False-lr5e-05-scFalse-wd0.0-bs128.bin')\n",
        "    )\n",
        "test_df = pd.read_csv('test_no_label.csv')\n",
        "test_dataset = test_df['Id']\n",
        "test = make_id_file_test(tokenizer, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZi14gnnrvgY"
      },
      "outputs": [],
      "source": [
        "class SentimentTestDataset(object):\n",
        "    def __init__(self, tokenizer, test):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        for sent in test:\n",
        "            self.data += [self._cast_to_int(sent.strip().split())]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erHjGE9rrvgY"
      },
      "outputs": [],
      "source": [
        "test_dataset = SentimentTestDataset(tokenizer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y03-nDX9rvgZ"
      },
      "outputs": [],
      "source": [
        "def collate_fn_style_test(samples):\n",
        "    input_ids = samples\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "    sorted_indices = range(len(input_ids))\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ0l1HparvgZ"
      },
      "outputs": [],
      "source": [
        "test_batch_size = 32\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
        "                                          shuffle=False, collate_fn=collate_fn_style_test,\n",
        "                                          num_workers=2, sampler=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZayKlYDM5FBQ",
        "outputId": "3454609d-75c2-4bf1-8943-dab1261c841d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Val: 100%|██████████| 32/32 [00:02<00:00, 10.82it/s]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    sentences = []\n",
        "    predictions = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Val',\n",
        "                                                                        position=0,\n",
        "                                                                        leave=True):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "        output = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "\n",
        "        logits = output.logits\n",
        "        batch_predictions = [0 if logit < 0.5 else 1 for logit in logits]\n",
        "        predictions += batch_predictions\n",
        "\n",
        "        for input_id in input_ids:\n",
        "            sentences.append(tokenizer.decode(input_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGO3aS-VrvgZ"
      },
      "outputs": [],
      "source": [
        "test_df['Category'] = predictions\n",
        "test_df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_and_eval.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
