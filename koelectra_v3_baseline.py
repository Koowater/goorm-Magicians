# -*- coding: utf-8 -*-
"""koelectra_v3_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AE3KMkwhQcxNcTeTF4W10HWjXHkxcjQ6
"""

!pip install transformers
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

from typing import List, Tuple, Dict, Any
import json
import random

class KoMRC:
    def __init__(self, data, indices: List[Tuple[int, int, int]]):
        self._data = data
        self._indices = indices

    # Json을 불러오는 메소드
    @classmethod
    def load(cls, file_path: str):
        with open(file_path, 'r', encoding='utf-8') as fd:
            data = json.load(fd)

        indices = []
        for d_id, document in enumerate(data['data']):
            for p_id, paragraph in enumerate(document['paragraphs']):
                for q_id, _ in enumerate(paragraph['qas']):
                    indices.append((d_id, p_id, q_id))
        
        return cls(data, indices)

    # 데이터 셋을 잘라내는 메소드
    @classmethod
    def split(cls, dataset, eval_ratio: float=.1, seed=42):
        indices = list(dataset._indices)
        random.seed(seed)
        random.shuffle(indices)
        train_indices = indices[int(len(indices) * eval_ratio):]
        eval_indices = indices[:int(len(indices) * eval_ratio)]

        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)

    def __getitem__(self, index: int) -> Dict[str, Any]:
        d_id, p_id, q_id = self._indices[index]
        paragraph = self._data['data'][d_id]['paragraphs'][p_id]

        context = paragraph['context']
        qa = paragraph['qas'][q_id]

        guid = qa['guid']
        question = qa['question']
        answers = qa['answers']

        return {
            'guid': guid,
            'context': context,
            'question': question,
            'answers': answers
        }

    def __len__(self) -> int:
        return len(self._indices)

dataset = KoMRC.load('train.json')
print("Number of Samples:", len(dataset))
print(dataset[0])

train_dataset, dev_dataset = KoMRC.split(dataset)
print("Number of Train Samples:", len(train_dataset))
print("Number of Dev Samples:", len(dev_dataset))
print(dev_dataset[0])

tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-base-v3-finetuned-korquad")

class Tokenization(KoMRC):
    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:
        super().__init__(data, indices)

    def __getitem__(self, index: int) -> Dict[str, Any]:
        sample = super().__getitem__(index)

        merge_sentence = sample['question'] + '[SEP]' + sample['context']
        len_question = len(tokenizer(sample['question'])['input_ids'])

        input_ids = tokenizer(merge_sentence, max_length=512)['input_ids'] # **max_length**
        attention_mask = [1]*len(input_ids) 
        token_type_ids = [0]*len_question + [1]*(len(input_ids)-len_question) # [CLS] question [SEP] to [0]

        if sample['answers']:
            answer = sample['answers'][0] # using only zero index
            target = tokenizer.decode(tokenizer(answer['text'])['input_ids'][1:-1]).replace(' ','')

            source = ''
            for end in range(len_question,len(input_ids)):
                source += tokenizer.decode(input_ids[end]).strip('#')
                if target in source:
                    break
            #else:
            #    print(target)
            #    raise ValueError("No mathced start position")

            source = ''
            for start in range(end+1)[::-1]:
                source = tokenizer.decode(input_ids[start]).strip('#') + source
                if target in source:
                    break
            #else:
            #    print(target)
            #    raise ValueError("No mathced start position")

            if start == 0 and end == 511: # max_length로 정답 token이 잘렸을 때 start, end를 511로 고정
                start = end

        else:
            start = None
            end = None
            answer = None
        
        return {
            'guid': sample['guid'],
            'input_ids': input_ids,
            'attention_mask': attention_mask, 
            'token_type_ids': token_type_ids,
            'start': start,
            'end': end,
            'context': sample['context'],
            'question': sample['question'],
            'answer': answer
        }

dataset = Tokenization.load('train.json')
train_dataset, dev_dataset = Tokenization.split(dataset, eval_ratio=.2)
print("Number of Train Samples:", len(train_dataset))
print("Number of Dev Samples:", len(dev_dataset))
print(dev_dataset[3])

sample = dev_dataset[0]
print(tokenizer.decode(tokenizer(sample['question'] + '[SEP]' + sample['context'])['input_ids'][sample['start']:sample['end']+1]))
print(sample['answer']['text'])

import torch
from torch.nn.utils.rnn import pad_sequence

def collator(samples):
    samples = {key : [sample[key] for sample in samples] for key in samples[0]}

    for key in 'start', 'end':
        if samples[key][0] is None:
            samples[key] = None
        else:
            samples[key] = torch.tensor(samples[key], dtype=torch.long)

    for key in 'input_ids', 'attention_mask', 'token_type_ids':
        samples[key] = pad_sequence(
            [torch.tensor(sample, dtype=torch.long) for sample in samples[key]],
            batch_first=True, padding_value=0 # 
        )
    return samples

from torch.utils.data import DataLoader

batch_size = 64
accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자

train_loader = DataLoader(train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)
dev_loader = DataLoader(dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)

batch = next(iter(train_loader))
print(batch['input_ids'].shape)
print(batch['input_ids'])
print(list(batch.keys()))

torch.manual_seed(42)
model = AutoModelForQuestionAnswering.from_pretrained('monologg/koelectra-base-v3-finetuned-korquad')
model.cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

import os
from statistics import mean

import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from tqdm.notebook import tqdm

os.makedirs('dump', exist_ok=True)
train_losses = []
dev_losses = []

step = 0

for epoch in range(1, 11):
    print("Epoch", epoch)
    # Training
    running_loss = 0.
    losses = []
    progress_bar = tqdm(train_loader, desc='Train')
    for batch in progress_bar:
        del batch['guid'], batch['context'], batch['question'], batch['answer']
        batch = {key: value.cuda() for key, value in batch.items()}
        start = batch.pop('start')
        end = batch.pop('end')
        
        start_logits, end_logits = model(**batch, return_dict=False)
        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)
        (loss / accumulation).backward()
        running_loss += loss.item()
        del batch, start, end, start_logits, end_logits, loss
        
        step += 1
        if step % accumulation:
            continue

        clip_grad_norm_(model.parameters(), max_norm=1.)
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

        losses.append(running_loss / accumulation)
        running_loss = 0.
        progress_bar.set_description(f"Train - Loss: {losses[-1]:.3f}")
    train_losses.append(mean(losses))
    print(f"train score: {train_losses[-1]:.3f}")

    # Evaluation
    losses = []
    for batch in tqdm(dev_loader, desc="Evaluation"):
        del batch['guid'], batch['context'], batch['question'], batch['answer']
        batch = {key: value.cuda() for key, value in batch.items()}
        start = batch.pop('start')
        end = batch.pop('end')
        
        with torch.no_grad():
            start_logits, end_logits = model(**batch, return_dict=False)
        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)

        losses.append(loss.item())
        del batch, start, end, start_logits, end_logits, loss
    dev_losses.append(mean(losses))
    print(f"Evaluation score: {dev_losses[-1]:.3f}")

    model.save_pretrained(f'dump/model.{epoch}')

import matplotlib.pyplot as plt

t = list(range(1, 11))
plt.plot(t, train_losses, label="Train Loss")
plt.plot(t, dev_losses, label="Dev Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

model = AutoModelForQuestionAnswering.from_pretrained('dump/model.10')
model.cuda()
model.eval()

for idx, sample in zip(range(1, 4), train_dataset):
    print(f'------{idx}------')
    print('Context:', sample['context'])
    print('Question:', sample['question'])
    
    input_ids, token_type_ids = [
        torch.tensor(sample[key], dtype=torch.long, device="cuda")
        for key in ("input_ids", "token_type_ids")
    ]
    
    with torch.no_grad():
        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :], return_dict=False)
    start_logits.squeeze_(0), end_logits.squeeze_(0)
    
    #start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)
    #end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)
    #probability = torch.triu(start_prob[:, None] @ end_prob[None, :])
    #index = torch.argmax(probability).item()
    #
    #start = index // len(end_prob)
    #end = index % len(end_prob)

    start = torch.argmax(start_logits).item()
    end = torch.argmax(end_logits).item()
    
    print('Predicted:',tokenizer.decode(tokenizer(sample['question'] + '[SEP]' + sample['context'])['input_ids'][start:end+1]))
    print('Answer:', tokenizer.decode(tokenizer(sample['question'] + '[SEP]' + sample['context'])['input_ids'][sample['start']:sample['end']+1]))
    #start = sample['position'][start][0]
    #end = sample['position'][end][1]
    #print('Answer:', sample['context'][start:end])

test_dataset = Tokenization.load('test.json')
print("Number of Test Samples", len(test_dataset))
print(test_dataset[0])

import csv

os.makedirs('out', exist_ok=True)
with torch.no_grad(), open('out/baseline.csv', 'w', encoding = 'utf-8-sig') as fd:
    writer = csv.writer(fd)
    writer.writerow(['Id', 'Predicted'])

    rows = []
    for sample in tqdm(test_dataset, "Testing"):
        input_ids, token_type_ids = [
            torch.tensor(sample[key], dtype=torch.long, device="cuda")
            for key in ("input_ids", "token_type_ids")
        ]
    
        with torch.no_grad():
            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :], return_dict=False)
        start_logits.squeeze_(0), end_logits.squeeze_(0)
    
        #start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)
        #end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)
        #probability = torch.triu(start_prob[:, None] @ end_prob[None, :])
        #index = torch.argmax(probability).item()
 
        #start = index // len(end_prob)
        #end = index % len(end_prob)

        #start = sample['position'][start][0]
        #end = sample['position'][end][1]
        
        start = torch.argmax(start_logits).item()
        end = torch.argmax(end_logits).item()

        predict = tokenizer.decode(tokenizer(sample['question'] + '[SEP]' + sample['context'])['input_ids'][start:end+1])
        if end - start > 10:
            predict = ''

        #rows.append([sample["guid"], sample['context'][start:end]])
        rows.append([sample['guid'], predict])
    
    writer.writerows(rows)

print(rows)