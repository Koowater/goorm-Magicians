{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "edit_distance",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMT1TinbabD7nXGhbz9XIb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Koowater/goorm-Magicians/blob/main/edit_distance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###편집거리 함수 정의"
      ],
      "metadata": {
        "id": "GHhonRO3LHAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit(s1, s2, debug=False):\n",
        "    if len(s1) < len(s2):\n",
        "        return edit(s2, s1, debug)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "\n",
        "        if debug:\n",
        "            print(current_row[1:])\n",
        "\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]"
      ],
      "metadata": {
        "id": "nlXtQQ0KOezp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습할때 loss와 함께 편집거리 계산"
      ],
      "metadata": {
        "id": "i9R2EFpOM3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from statistics import mean\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "os.makedirs('dump', exist_ok=True)\n",
        "train_epoch=2  #train_epoch설정\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "dev_edit_ds=[]\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in range(train_epoch): \n",
        "    print(\"Epoch\", epoch)\n",
        "    # Training\n",
        "    running_loss = 0.\n",
        "    losses = []\n",
        "    progress_bar = tqdm(train_loader, desc='Train')\n",
        "    for batch in progress_bar:\n",
        "        del batch['guid'], batch['context'], batch['question'], batch['answer'] #동영님 코드에는 batch[\"offsets\"] 추가\n",
        "        batch = {key: value.cuda() for key, value in batch.items()}\n",
        "        start = batch.pop('start')\n",
        "        end = batch.pop('end')\n",
        "        \n",
        "        start_logits, end_logits = model(**batch, return_dict=False)\n",
        "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
        "        (loss / accumulation).backward()\n",
        "        running_loss += loss.item()\n",
        "        del batch, start, end, start_logits, end_logits, loss\n",
        "        \n",
        "        step += 1\n",
        "        if step % accumulation:\n",
        "            continue\n",
        "\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        losses.append(running_loss / accumulation)\n",
        "        running_loss = 0.\n",
        "        progress_bar.set_description(f\"train_loss: {losses[-1]:.3f}\")\n",
        "    train_losses.append(mean(losses))\n",
        "    #wandb.log({'train_loss': mean(train_losses)})  #wandb 기록\n",
        "    print(f\"train score: {train_losses[-1]:.3f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    losses = []\n",
        "    edit_ds=[]\n",
        "    for batch in tqdm(dev_loader, desc=\"evaluation\"): #batch는 16개씩 모아놓은 세트(batch_size=64, accumulation=4 인 경우)\n",
        "        for i in range(len(batch[\"question\"])): #len(batch[\"question\"])=64/4=16\n",
        "          input_ids, token_type_ids=[\n",
        "              torch.tensor(batch[key][i],dtype=torch.long, device='cuda')\n",
        "              for key in (\"input_ids\", \"token_type_ids\")\n",
        "              ]\n",
        "          with torch.no_grad():\n",
        "              start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :], return_dict=False)\n",
        "          start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "\n",
        "          start = torch.argmax(start_logits).item()\n",
        "          end = torch.argmax(end_logits).item()\n",
        "\n",
        "          y_true=tokenizer.decode(tokenizer(batch['question'][i] + '[SEP]' + batch['context'][i])[\"input_ids\"][batch['start'][i]:batch['end'][i]+1])\n",
        "          y_pred=tokenizer.decode(tokenizer(batch['question'][i] + '[SEP]' + batch['context'][i])['input_ids'][start:end+1]) \n",
        "          if end-start>10:\n",
        "            y_pred=\"\"\n",
        "          edit_ds.append(edit(y_pred, y_true))\n",
        "        del batch['guid'], batch['context'], batch['question'], batch['answer'] #동영님 코드에는 batch[\"offsets\"] 추가\n",
        "        batch = {key: value.cuda() for key, value in batch.items()} \n",
        "        s = batch.pop('start') \n",
        "        e = batch.pop('end') \n",
        "        \n",
        "        with torch.no_grad():\n",
        "          start_logits, end_logits = model(**batch, return_dict=False) \n",
        "          start = torch.argmax(start_logits).item()\n",
        "          end = torch.argmax(end_logits).item()\n",
        "        loss = F.cross_entropy(start_logits, s) + F.cross_entropy(end_logits, e)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        del batch, start, end, start_logits, end_logits, loss, s, e\n",
        "        \n",
        "        \n",
        "    dev_losses.append(mean(losses))\n",
        "    dev_edit_ds.append(mean(edit_ds))\n",
        "    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n",
        "    print(f\"dev_edit_distance: {dev_edit_ds[-1]:.3f}\")\n",
        "    #wandb에 기록\n",
        "    #wandb.log({'dev_loss': mean(dev_losses), 'dev_edit_distance': mean(dev_edit_ds)})\n",
        "    model.save_pretrained(f'dump/model.{epoch}')   "
      ],
      "metadata": {
        "id": "hPk4SrH7JDJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###학습시킨 후에 dev dataset에 대해서만 편집거리 계산"
      ],
      "metadata": {
        "id": "gA7kSf8iMi24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MacxEFMIIbG9"
      },
      "outputs": [],
      "source": [
        "edit_ds=[]\n",
        "for idx, sample in zip(range(len(dev_dataset)), dev_dataset):  \n",
        "    input_ids, token_type_ids = [\n",
        "        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n",
        "        for key in (\"input_ids\", \"token_type_ids\")\n",
        "    ]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :], return_dict=False)\n",
        "    start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "\n",
        "\n",
        "    start = torch.argmax(start_logits).item()\n",
        "    end = torch.argmax(end_logits).item()\n",
        "\n",
        "    y_true=tokenizer.decode(tokenizer(sample['question'] + '[SEP]' + sample['context'])['input_ids'][sample['start']:sample['end']+1])\n",
        "    y_pred=tokenizer.decode(tokenizer(sample['question'] + '[SEP]' + sample['context'])['input_ids'][start:end+1])\n",
        "    if end-start>10:\n",
        "      y_pred=''\n",
        "\n",
        "    edit_ds.append(edit(y_pred, y_true))\n",
        "    dev_edit_distance=mean(edit_ds)\n",
        "\n",
        "print(f\"dev_edit_distance: {mean(edit_ds):.3f}\")"
      ]
    }
  ]
}