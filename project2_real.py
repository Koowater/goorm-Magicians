# -*- coding: utf-8 -*-
"""Project2_real.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MaOntDhS1y10veomAD8LwyNEbWnp8RoR
"""

! apt-get install -y openjdk-8-jdk python3-dev
! pip install konlpy "tweepy<4.0.0"
! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

!pip install transformers

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('monologg/koelectra-base-v2-finetuned-korquad')

print(tokenizer.tokenize('나는 학교에 갔다'))
tokenizer('나는 학교에 갔다')['input_ids']

from typing import List, Tuple, Dict, Any
import json
import random

class KoMRC:
    def __init__(self, data, indices):
        self._data = data
        self._indices = indices

    @classmethod
    def load(cls, file_path: str):
        with open(file_path, 'r', encoding='utf-8') as fd:
            data = json.load(fd)

        indices = []
        for d_id, document in enumerate(data['data']):
            for p_id, paragraph in enumerate(document['paragraphs']):
                for q_id, _ in enumerate(paragraph['qas']):
                    indices.append((d_id, p_id, q_id))

        return cls(data, indices)

    @classmethod
    def split(cls, dataset, eval_ratio: float=.1, seed=42):
      indices = list(dataset._indices)
      random.seed(seed)
      random.shuffle(indices)
      train_indices = indices[int(len(indices) * eval_ratio):]
      eval_indices = indices[:int(len(indices) * eval_ratio)]

      return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)

    def __getitem__(self, index: int) -> Dict[str, Any]:
        d_id, p_id, q_id = self._indices[index]
        paragraph = self._data['data'][d_id]['paragraphs'][p_id]

        context = paragraph['context']
        qa = paragraph['qas'][q_id]

        guid = qa['guid']
        question = qa['question']
        answers = qa['answers']

        return {
            'guid': guid,
            'context': context,
            'question': question,
            'answers': answers
        }

    def __len__(self) -> int:
      return len(self._indices)

dataset = KoMRC.load('./sample_data/train.json')
print("Number of Samples: ", len(dataset))
dataset[0] # 첫 번째 qeustion

train_dataset, dev_dataset = KoMRC.split(dataset)
print("Number of Train Samples:", len(train_dataset))
print("Number of Dev Samples:", len(dev_dataset))

from typing import Generator
import konlpy

class TokenizedKoMRC(KoMRC):
    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:
        super().__init__(data, indices)
        self._tagger = konlpy.tag.Mecab()

    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:
        position = 0
        tokens = []
        for morph in self._tagger.morphs(sentence):
            position = sentence.find(morph, position)
            tokens.append((morph, (position, position + len(morph))))
            position += len(morph)
        return tokens
            
    def __getitem__(self, index: int) -> Dict[str, Any]:
        sample = super().__getitem__(index)

        context, position = zip(*self._tokenize_with_position(sample['context']))
        context, position = list(context), list(position)
        question = self._tagger.morphs(sample['question'])

        if sample['answers'] is not None:
            answers = []
            for answer in sample['answers']:
                for start, (position_start, position_end) in enumerate(position):
                    if position_start <= answer['answer_start'] < position_end:
                        break
                else:
                    print(context, answer)
                    raise ValueError("No mathced start position")

                target = ''.join(answer['text'].split(' '))
                source = ''
                for end, morph in enumerate(context[start:], start):
                    source += morph
                    if target in source:
                        break
                else:
                    print(context, answer)
                    raise ValueError("No Matched end position")

                answers.append({
                    'start': start,
                    'end': end
                })
        else:
            answers = None
        
        return {
            'guid': sample['guid'],
            'context_original': sample['context'],
            'context_position': position,
            'question_original': sample['question'],
            'context': context,
            'question': question,
            'answers': answers
        }

dataset = TokenizedKoMRC.load('./sample_data/train.json')

train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)
print("Number of Train Samples:", len(train_dataset))
print("Number of Dev Samples:", len(dev_dataset))
print(dev_dataset[0])

dataset[0]['answers']

from typing import Sequence
from collections import Counter
from itertools import chain

from tqdm.notebook import tqdm

class Indexer:
    def __init__(self, 
        id2token: List[str], 
        max_length: int=512,  ############################################
        pad: str='<pad>', unk: str='<unk>', cls: str='<cls>', sep: str='<sep>'
      ):
        self.pad = pad
        self.unk = unk
        self.cls = cls
        self.sep = sep
        self.special_tokens = [pad, unk, cls, sep]

        self.max_length = max_length

        self.id2token = self.special_tokens + id2token
        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}

    @property
    def vocab_size(self):
        return len(self.id2token)
    
    @property
    def pad_id(self):
        return self.token2id[self.pad]
    @property
    def unk_id(self):
        return self.token2id[self.unk]
    @property
    def cls_id(self):
        return self.token2id[self.cls]
    @property
    def sep_id(self):
        return self.token2id[self.sep]

    @classmethod
    def build_vocab(cls,
        dataset: TokenizedKoMRC, 
        min_freq: int=5
    ):
        counter = Counter(chain.from_iterable(
            sample['context'] + sample['question']
            for sample in tqdm(dataset, desc="Counting Vocab")
        ))

        return cls([word for word, count in counter.items() if count >= min_freq])
    
    def decode(self,
        token_ids: Sequence[int]
    ):
        return [self.id2token[token_id] for token_id in token_ids]

    def sample2ids(self,
        sample: Dict[str, Any],
    ) -> Dict[str, Any]:
        context = [self.token2id.get(token, self.unk_id) for token in sample['context']]
        question = [self.token2id.get(token, self.unk_id) for token in sample['question']]

        context = context[:self.max_length - len(question) - 3]             # Truncate context
        
        
        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]
        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)
        
        if sample['answers'] is not None:
            answer = sample['answers'][0]
            start = min(answer['start'] + len(question) + 2, self.max_length - 1)
            end = min(answer['end'] + len(question) + 2, self.max_length - 1)
        else:
            start = None
            end = None

        return {
            'guid': sample['guid'],
            'context': sample['context_original'],
            'question': sample['question_original'],
            'position': sample['context_position'],
            'input_ids': input_ids,
            'token_type_ids': token_type_ids,
            'start': start,
            'end': end
        }

indexer = Indexer.build_vocab(dataset)

a1 = '나는 학교에 간다'
a2 = '집 가고 싶다'

question_tokens = tokenizer(a1)['input_ids']
context_tokens = tokenizer(a2)['input_ids']
token_type_ids = [0]*(len(question_tokens)-1) + [1]*len(context_tokens)
print(question_tokens)
print(context_tokens)
print(token_type_ids)

##########################################
class Data2:
  def __init__(self, sample, tokenizer):
    self.sample = sample
    self.tokenizer = tokenizer
  
  def __len__(self) -> int:
    return len(self.sample)

  def __getitem__(self, index) -> Dict[str, Any]:
    question = self.sample[index]['question_original']
    context = self.sample[index]['context_original']
    full_sentence = self.sample[index]['question_original'] + '[SEP]' + self.sample[index]['context_original']
    
    input_ids = self.tokenizer(full_sentence, max_length=512, truncation=True)['input_ids']
    
    question_tokens = tokenizer(self.sample[index]['question_original'])['input_ids']
    context_tokens = tokenizer(self.sample[index]['context_original'])['input_ids']
    token_type_ids = [0]*(len(question_tokens)-1) + [1]*len(context_tokens)
    token_type_ids = token_type_ids[:512]
   #token_type_ids = self.tokenizer(full_sentence, max_length=512, truncation=True)['token_type_ids'] -> 이렇게 넣으면 다 0으로 나옴 ..

    attention_mask = self.tokenizer(full_sentence, max_length=512, truncation=True)['attention_mask']


    answer = self.sample[index]['answers'][0]
    start = min(answer['start'] + len(question) + 2, 512 - 1)
    end = min(answer['end'] + len(question) + 2, 512 - 1)

    ############################ 추가 → 4개 안 넣으니까 length 오류 나옴 ..
    guid = self.sample[index]['guid']
    context = self.sample[index]['context_original']
    question = self.sample[index]['question_original']
    position = self.sample[index]['context_position']
    
    return {'guid': guid,
            'context': context,
            'input_ids': input_ids,
            'question': question,
            'position': position,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids,
            'start': start, 
            'end': end}

train_loader_set = Data2(train_dataset, tokenizer)
val_loader_set = Data2(dev_dataset, tokenizer)

import torch
from torch.nn.utils.rnn import pad_sequence

class Collator:
    def __init__(self, indexer: Indexer) -> None:
        self._indexer = indexer

    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        samples = {
            key: [sample[key] for sample in samples]
            for key in samples[0]
        }

        for key in 'start', 'end':
            if samples[key][0] is None:
                samples[key] = None
            else:
                samples[key] = torch.tensor(samples[key], dtype=torch.long)
        for key in 'input_ids', 'attention_mask', 'token_type_ids':
            samples[key] = pad_sequence(
                [torch.tensor(sample, dtype=torch.long) for sample in samples[key]],
                batch_first=True, padding_value=self._indexer.pad_id
            )

        return samples

from torch.utils.data import DataLoader

batch_size = 64
accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자

collator = Collator(indexer)
train_loader = DataLoader(train_loader_set, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)
dev_loader = DataLoader(val_loader_set, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)

batch = next(iter(dev_loader))
print(batch['input_ids'].shape)
print(batch['input_ids'])
print(list(batch.keys()))

import torch
from transformers import AutoModelForQuestionAnswering

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

torch.manual_seed(42)
model = AutoModelForQuestionAnswering.from_pretrained('monologg/koelectra-base-v2-finetuned-korquad')
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

model.config

import os
from statistics import mean

import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_

os.makedirs('dump', exist_ok=True)
train_losses = []
dev_losses = []

step = 0

for epoch in range(10): ################## -> 원래 epoch=30
    print("Epoch", epoch)
    # Training
    running_loss = 0.
    losses = []
    progress_bar = tqdm(train_loader, desc='Train')
    for batch in progress_bar:
        del batch['guid'], batch['context'], batch['question'], batch['position'] 
        batch = {key: value.to(device) for key, value in batch.items()}
        start = batch.pop('start')
        end = batch.pop('end')
        
        start_logits, end_logits = model(**batch, return_dict=False) ################## cross_entropy 계산시 return_dict=False 필수!

        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)
        (loss / accumulation).backward()
        running_loss += loss.item()
        del batch, start, end, start_logits, end_logits, loss
        
        step += 1
        if step % accumulation:
            continue

        clip_grad_norm_(model.parameters(), max_norm=1.)
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

        losses.append(running_loss / accumulation)
        running_loss = 0.
        progress_bar.set_description(f"Train - Loss: {losses[-1]:.3f}")
    train_losses.append(mean(losses))
    print(f"train score: {train_losses[-1]:.3f}")

    # Evaluation
    losses = []
    for batch in tqdm(dev_loader, desc="Evaluation"):
        del batch['guid'], batch['context'], batch['question'], batch['position']
        batch = {key: value.to(device) for key, value in batch.items()}
        start = batch.pop('start')
        end = batch.pop('end')
        
        with torch.no_grad():
            start_logits, end_logits = model(**batch, return_dict=False) #####################
        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)

        losses.append(loss.item())
        del batch, start, end, start_logits, end_logits, loss
    dev_losses.append(mean(losses))
    print(f"Evaluation score: {dev_losses[-1]:.3f}")

    model.save_pretrained(f'dump/model.{epoch}')

import matplotlib.pyplot as plt

t = list(range(10))
plt.plot(t, train_losses, label="Train Loss")
plt.plot(t, dev_losses, label="Dev Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

from transformers import ElectraForQuestionAnswering

model.load_state_dict(torch.load('./dump/model.1/pytorch_model.bin'))
model.eval()

class IndexerWrappedDataset:
    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:
        self._dataset = dataset
        self._indexer = indexer

    def __len__(self) -> int:
        return len(self._dataset)
    
    def __getitem__(self, index: int) -> Dict[str, Any]:
        sample = self._indexer.sample2ids(self._dataset[index])
        sample['attention_mask'] = [1] * len(sample['input_ids'])

        return sample

indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)

for idx, sample in zip(range(1, 4), train_loader_set):
    print(f'------{idx}------')
    print('Context:', sample['context'])
    print('Question:', sample['question'])
    
    input_ids, token_type_ids = [
        torch.tensor(sample[key], dtype=torch.long, device="cuda")
        for key in ("input_ids", "token_type_ids")
    ]
    
    with torch.no_grad():
        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :], return_dict=False) ###### ???????
    start_logits.squeeze_(0), end_logits.squeeze_(0)
    
    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)
    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)
    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])
    index = torch.argmax(probability).item()
    
    start = index // len(end_prob)
    end = index % len(end_prob)
    
    start = sample['position'][start][0]
    end = sample['position'][end][1]

    print('Answer:', sample['context'][start:end])

##########################################
class Test_data2:
  def __init__(self, sample, tokenizer):
    self.sample = sample
    self.tokenizer = tokenizer
  
  def __len__(self) -> int:
    return len(self.sample)

  def __getitem__(self, index) -> Dict[str, Any]:
    question = self.sample[index]['question_original']
    context = self.sample[index]['context_original']
    full_sentence = self.sample[index]['question_original'] + '[SEP]' + self.sample[index]['context_original']
    
    input_ids = self.tokenizer(full_sentence, max_length=512, truncation=True)['input_ids']

    question_tokens = tokenizer(self.sample[index]['question_original'])['input_ids']
    context_tokens = tokenizer(self.sample[index]['context_original'])['input_ids']
    token_type_ids = [0]*(len(question_tokens)-1) + [1]*len(context_tokens)
    token_type_ids = token_type_ids[:512]

    attention_mask = self.tokenizer(full_sentence, max_length=512, truncation=True)['attention_mask']

    ############################ 추가
    guid = self.sample[index]['guid']
    context = self.sample[index]['context_original']
    question = self.sample[index]['question_original']
    position = self.sample[index]['context_position']
    
    return {'guid': guid,
            'context': context,
            'input_ids': input_ids,
            'question': question,
            'position': position,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids}

test_dataset = TokenizedKoMRC.load('./sample_data/test.json')
#test_dataset = IndexerWrappedDataset(test_dataset, indexer)
test_loader_set = Test_data2(test_dataset, tokenizer)
print("Number of Test Samples", len(test_loader_set))
print(test_loader_set[0])

import csv

os.makedirs('out', exist_ok=True)
with torch.no_grad(), open('out/baseline.csv', 'w') as fd:
    writer = csv.writer(fd)
    writer.writerow(['Id', 'Predicted'])

    rows = []
    for sample in tqdm(test_loader_set, "Testing"):
        input_ids, token_type_ids = [
            torch.tensor(sample[key], dtype=torch.long, device="cuda")
            for key in ("input_ids", "token_type_ids")
        ]
    
        with torch.no_grad():
            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :], return_dict=False) #########
        start_logits.squeeze_(0), end_logits.squeeze_(0)
    
        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)
        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)
        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])
        index = torch.argmax(probability).item()
    
        start = index // len(end_prob)
        end = index % len(end_prob)
        try: ######################
          start = sample['position'][start][0]
          end = sample['position'][end][1]
          rows.append([sample["guid"], sample['context'][start:end]])
        except:
          rows.append([sample['guid'], ''][start:end]]) ################ max_length 넘어가면 그냥 공백으로 처리 -> 어떻게 처리할지..?
    
    writer.writerows(rows)